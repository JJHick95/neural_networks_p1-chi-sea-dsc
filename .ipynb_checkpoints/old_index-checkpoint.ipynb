{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from src.student_caller import one_random_student, three_random_students\n",
    "from src.student_list import student_first_names\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks draw their inspiration from the biology of our own brains, which are of course also accurately described as 'neural networks'. A human brain contains around $10^{11}$ neurons, connected very **densely**.\n",
    "\n",
    "Neural nets do not have quite so many neurons. They can have, as shown below, several layers of densley connected neurons.  The dense connections relay information between the layers. Each neuron outputs a signal to the neuron in the successive layer.  The message travels through the network and ultimately results in a decision. In the case below, the network correctly categorizes the picture as a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dense](img/dogcat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks have been around for a while. They are over 70 years old, dating back to  their proposal in 1944 by Warren McCullough and Walter Pitts. These first proposed neural nets had thresholds and weights, but no layers and no specific training mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Perceptron\" the first trainable neural network was created by Frank Rosenblatt in 1957. It consisted of a single layer with adjustable weights in the middle of input and output layers.  The perceptron in its original form took binary inputs and predicted a binary outcome.  Similar to a biological neuron, if the sum of the inputs multiplied by the weights exceeded a certain threshold value, the neuron would fire. In the first perceptron, this 'firing' was represented by the positive binary output, 1. The activation function looked like so:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be rewritten as:\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b, which is equal to -threshold, can be thought of as a term regulating how easily the neuron fires.  If b is a large negative number, the output of ${w}\\cdot{x}$ would need to be very large to output a 1.  If b is a large positive number, the value of ${w}\\cdot{x}$ could be relatively small to output 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above perceptron is that it does not allow for an effective way of learning. In order to build effective learning algorithms, we require the ability to tune the output of our neuron with slight changes to our weights and biases.  Thinking about the activation function above, that is not the case.  A small change of a weight will most often result in the same output as the original weight resulted in. Only in the scenario when the small change pushes the output above the threshold does the output flip from 0 to 1, or vice-versa.  If this were to occur, a slight change in the weight result in complete reversal of the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to effect a system where small changes in weights effect small changes in the output and thereby allowing our system to learn, we have to change our activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid neurons are very similar to perceptrons, but instead of an all-or-none activation function, they use sigmoid activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered the sigmoid function way back at the beginning of phase 3 during logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large \\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "def sigmoid(z):\n",
    "    \n",
    "    '''\n",
    "    The sigmoid activation function for a single node of a neural net\n",
    "    parameters:\n",
    "        z: the result of a linear equation consisting of a set of feature inputs\n",
    "        multiplied by a set of weights plus a bias.\n",
    "    returns:\n",
    "        a number between 0 and 1\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return 1/(1+math.e**(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input to the sigmoid function is large, the output will be very close to one.  If it is small, the output will be very close to zero.\n",
    "\n",
    "If we look at the range of inputs from large negative numbers to large positive, we see the output of the sigmoid function has the characteristic s-shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 10000 inputs ranging from -10 to 10\n",
    "inputs = np.linspace(-10,10, 1000)\n",
    "\n",
    "# Convert the inputs to a number between 0 and 1\n",
    "outputs = [sigmoid(z) for z in inputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Output of the sigmoid activation function')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(inputs, outputs, ax=ax)\n",
    "ax.set_title('Output of the sigmoid activation function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propogation refers to the process of converting an input signal into a prediction. \n",
    "\n",
    "The features are multiplied by a set of weights, summed along with a bias, passed into the sigmoid function, and converted to a prediction with a threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we will use the built in dataset of handwritten numbers from sklearn, which comes from the UCI Machine Learning collection [digits source](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits). Each record is a 8 by 8 bit image of a handwritten number between 0 and 9. Each pixel value (a number between 0 and 16) represents the relative brightness of the pixel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "flat_image = np.array(digits.data[0]).reshape(digits.data[0].shape[0], -1)\n",
    "eight_by_eight_image = digits.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is similar to the famous [**MNIST**](http://yann.lecun.com/exdb/mnist/index.html) dataset which is sometimes referred to the \"hello world\" of computer vision [source](https://www.kaggle.com/c/digit-recognizer).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one input of pixels from a number, our input/output process looks like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "eight_by_eight_image = digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "imgplot = plt.imshow(eight_by_eight_image, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the matrix below and make sure you see how the large numbers \n",
    "# correspond to darker shades in the image above\n",
    "eight_by_eight_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "\n",
    "# We will turn the target into a binary for demonstration purposes.\n",
    "target = (digits.target % 2 == 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most simple version, if our network has only 1 neuron with a sigmoid activation function, we can code a round of forward propogation very easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an image of a one layer network with a single neuron in the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simplenn](img/simple_nn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to our sigmoid neuron is simply the pixels of our image.\n",
    "\n",
    "We flip our general dataframe structure around a bit. Instead of features as columns, each image will take up a column.  Our pixels from each number will be stacked vertically, and our numbers will be arranged like books on a shelf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $x_0 = \\begin{bmatrix} 0  \\\\ 0 \\\\  \\vdots \\\\ 0 \\\\4 \\\\\\vdots \\\\ 0 \\\\ 0 \\\\ \\vdots \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "        [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "        [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "        [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "        [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "        [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "        [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "        [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "        [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "        [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "        [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "        [ 0.,  1.,  8., ..., 12.,  1.,  0.]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our original images come stacked on top of each other in rows\n",
    "digits.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.images.reshape(1797,64).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To show that the pixel arrangement is correct, let's plot all rows of the last column\n",
    "imgplot = plt.imshow(X[:,-1].reshape(8,8), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same as our last image\n",
    "imgplot = plt.imshow(digits.images[-1], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our weights vector will have the same number of weights as pixels. One weight per pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![weights](img/log-reg-nn-ex-w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will initialize our weights as random numbers around zero\n",
    "weights = np.random.uniform(-1,1,64).reshape(-1,1)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass our inputs and our weights into the summation function.  Each weight is multiplied by the pixel it is associated with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sum](img/log-reg-nn-ex-sum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily perform this in code with the dot product if we transpose our weights vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "summation = weights.T.dot(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with one summation for each image in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass these sums into our sigmoid activation function along with the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activation](img/log-reg-nn-ex-a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize bias as 0\n",
    "bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the bias\n",
    "a = sigmoid(summation+bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete one round of forward propogation for our simplest of neurons, we convert the output of the sigmoid to a binary outcome using a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 646, 1151]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = (a > .5).astype(int)\n",
    "np.unique(y_hat, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a1f84de48>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef = lr.coef_.reshape(8,-8)\n",
    "plt.imshow(coef, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When passing the data into our perceptron, we will flatten the image into a 64x1 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_image = np.array(digits.data[0]).reshape(digits.data[0].shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our weights vector will have the same number of weights as pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![weights](img/log-reg-nn-ex-w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will instantiate our weight with small random numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What shape should our weight matrix have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\"The number of pixels by the number of nodes: 64x1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.uniform(-.1, .1, (flat_image.shape[0],1))\n",
    "w[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set our bias term to 0: there is ony one for a singal perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sum](img/log-reg-nn-ex-sum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs, the pixel, each are multiplied by their respective weights and then summed together with the bias. \n",
    "\n",
    "This amounts to the dotproduct of the pixel value and the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = flat_image.T.dot(w) + b\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Why do we have to transpose our flat_image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "print(flat_image.shape)\n",
    "(w.shape)\n",
    "\n",
    "# to perform matrix multiplication, we require nxm dot mxn, \n",
    "# we need the column dimension of the left hand matrix to match the \n",
    "# to match the row dimension of the right hand matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![activation](img/log-reg-nn-ex-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pass it into an activation function. The activation function converts our summed inputs into an output, which is then passed on to other nodes in hidden layers, or as an end product in the output layer. This can looslely be thought of as the action potential traveling down the axon. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we build our models in Keras, we will specify the activation function of both hidden layers and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What is an activation function we have come across? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![don't look down](https://media.giphy.com/media/kGX9vntSO8McNlDaVj/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions play the role of converting our output to a specific form. The sigmoid function converts linear equation from a number that could be any number $-\\infty$ to $\\infty$, to a number between 0 and 1.  This conveniently allowed us to associate the output as a probability of a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z is the input from our collecter, the sum of the weights multiplied by the features and the bias\n",
    "\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Input the sum of our weights times the pixel intensities, plus the bias\n",
    "    Output a number between 0 and 1.\n",
    "    \n",
    "    '''\n",
    "    return 1/(1+np.e**(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sigmoid(z)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a suite of activation functions to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.linspace(-10, 10, 20000)\n",
    "sig = sigmoid(X)\n",
    "\n",
    "plt.plot(X, sig);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tanh**: $f(x) = tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh a shifted version of the sigmoid. The inflection point passes through 0,0 instead of 0,.5, and the output is between -1 and 1.  This means the mean of the output is centered around 0, which can make learning in the next layer easier.  tanh is almost always better in a **hidden layer** than the sigmoid because if speeds up learning [see here](https://stats.stackexchange.com/questions/330559/why-is-tanh-almost-always-better-than-sigmoid-as-an-activation-function). For the output layer, however, sigmoid makes sense for binary outcomes.  If we require an output of 0 or 1, it makes sense for the activation function to output between 0 and 1, rather than -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding tanh:\n",
    "\n",
    "X = np.linspace(-10, 10, 20000)\n",
    "y_tanh = (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
    "\n",
    "plt.plot(X, y_tanh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tanh(z)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with tanh (and sigmoid), is that if our input is large, then the slope of the activation function flattens out.  When conducting backpropogation, we will use the derivative of the activation function as one of our terms multiplied by a learning rate to determine how big a step to take when adjusting our weights.  If our derivative is close to zero, the step will be very small, so the speed of our learning will be very slow, which is a huge problem.  This is called the **vanishing gradient** problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-10, 10, 2000)\n",
    "y_tanh = (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
    "\n",
    "plt.plot(X, y_tanh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU, or rectified linear unit, outputs 0 for negative numbers, and the original value for positive inputs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU**: $f(x) = 0$ if $x\\leq 0$; $f(x) = x$ otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU is a commonly used and effective activation function because of speed.  Given that the **output** is zero when negative, some nodes become inactive (i.e. produce an output of 0).  Zero outputs take little computational power. Also, the constant gradient leads to faster learning in comparison to sigmoid and tanh, which come close to 0 with large positive and negative values.  Since the speed of our network is linked to the derivative, a derivative close to zero will result in very slow learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding ReLU:\n",
    "\n",
    "X = np.linspace(-10, 10, 200)\n",
    "\n",
    "y_relu = list(np.zeros(100))\n",
    "y_relu.extend(np.linspace(0, 10, 100))\n",
    "\n",
    "plt.plot(X, y_relu);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    if z <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = relu(z)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that ReLU (\"Rectified Linear Unit\") increases without bound as $x\\rightarrow\\infty$. The advantages and drawbacks of this are discussed on [this page on stackexchange](https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many activation functions, [see here](https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our nodes will be taking in input from multiple sources. Let's add the entire training set as our input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=42, test_size=.2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy allows us to easily calculate the predictions for the set of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What dimension should our weight vector now be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__ \n",
    "'''The same as before.  Each weight is associated with one pixel \n",
    "location across the entire training set'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What should be the dimension of the output of our collector function be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'''1437x1 one sum for every image'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_0 = X_train.dot(w)+b\n",
    "z_0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_0 = sigmoid(z_0)\n",
    "a_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_0_relu = [relu(z_0) for z_0 in z_0]\n",
    "a_0_relu[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our DS purposes, we'll generally imagine our network to consist of only a few layers, including an input layer (where we feed in our data) an output layer (comprising our predictions). Significantly, there will also (generally) be one or more layers of neurons between input and output, called **hidden layers**.\n",
    "\n",
    "One reason these are named hidden layers is that what their output actually represents in not really known.  The activation of node 1 of the first hidden layer may represent a sequence of pixel intensity corresponding to a horizontal line, or a group of dark pixels in the middle of a number's loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dense](img/Deeper_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are unaware of how exactly these hidden layers are operating, neural networks are considered **black box** algorithms.  You will not be able to gain much inferential insight from a neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add **one** hidden layer to our network with **four** nodes.\n",
    "\n",
    "Each of our pixels from our digit representation goes to each of our nodes, and each node has a set of weights and a bias term associated with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: what should the dimensions of our weight matrix be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "'''64x4 one weight for every pixel for each node'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = np.random.normal(0,.1, (X_train.shape[1],4))\n",
    "w_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = X_train.dot(w_1) + b_1\n",
    "z_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1 = sigmoid(z_1)\n",
    "a_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each of these neurons has a set of weights and a bias associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the shape of this weight matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_random_student(mccalister)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "# 4x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = np.random.normal(0,.1, (a_1.shape[1],1))\n",
    "\n",
    "w_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = a_1.dot(w_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sigmoid(z_1)\n",
    "y_pred = output > .5\n",
    "y_hat = y_pred.astype(int)\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation\n",
    "\n",
    "After a certain number of data points have been passed through the model, the weights will be *updated* with an eye toward optimizing our loss function. (Thinking back to biological neurons, this is like revising their activation potentials.) Typically, this is  done  by using some version of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bprop](img/BackProp_web.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "The loss function tells us how well our model performed by comparing the predictions to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our models with Keras, we will watch the loss function's progress across epochs.  A decreasing loss function will show us that our model is **improving**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is associated with the nature of our output. In logistic regression, our output was binary, so our loss function was the negative loglikelihood, aka **cross-entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\Large -\\ loglikelihood = -\\frac{1}{m} * \\sum\\limits_{i=1}^m y_i\\log{p_i} + (1-y_i)\\log(1-p_i) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train %2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train*np.log(output) + (1-y_train) * np.log(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "neg_ll = -1/len(y_train)*np.sum(y_train*np.log(output) + (1-y_train) * np.log(1-output))\n",
    "neg_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For continuous variables, the loss function we have relied on is [MSE or MAE](http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/).\n",
    "\n",
    "Good [resource](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) on backpropogation with RMSE loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a good summary of different [loss functions]( https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html):\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent, Epochs, and Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We not only use the the loss function to see our model is improving, we use it to update our parameters.  The gradient of the loss function is calculated in relation to each parameter of our neural net.\n",
    "\n",
    "$$\\large dw_1 = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d w_1} = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d \\hat y}\\displaystyle\\frac{d\\hat y}{dz}\\displaystyle\\frac{dz}{d w_1} = x_1 dz $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working through the Learn's Intro to Neural Networks will allow you to dive deep into the partial derivatives. For now, I will just point out that the derivative of the weight is multiplied by the derivative of our activation function, *$d\\hat{y}$*.  Here you can get a glimpse of the problem with the sigmoid/tanh as an activation function for a hidden layer.  Since the derivative of the sigmoid approaches zero for very large positive or negative numbers, the update to the parameters (the partial derivative multiplied by a learning rate ($ \\alpha $)) approaches zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_1 := w_1 - \\alpha dw_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of our neural net goes way down as a result, since the updates are so incrementally small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a deep dive into the fitting process, reference Chapter 11 in [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent can be performed in several different ways.  Unlike sklearn implimentation of linear regression, which finds the minimum of the loss with a closed form solution, neural networks move down the gradient **incrementally.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run our neural nets in Keras, we can set the hyperparameter verbose equal to 1, and we will see progress through **epochs**\n",
    "\n",
    "![epoch](img/2014-10-28_anthropocene.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each epoch, **all examples** from are training set have passed through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of gradient descent update the parameters at different times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is calculated across all values.  We can find the direction of the gradient, and proceed directly towards the minimum .\n",
    "\n",
    "The weights are updated with regard to the cost at the **end of an epoch** after all training elements have passed through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the weights after all training examples have passed through can be detrimentally slow.  \n",
    "\n",
    "SGD updates the weights after each training example. SGD requires less epochs to achieve quality coefficients. This speeds up gradient descent significantly [link](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "In mini-batch, we pass a batch, calculated the gradient, update the params, then proceed to the next batch.  It combines the advantages of batch and stochastic gradient descent: it is more faster than SGD since the updates are not made with each point, and more computationally efficient than batch, since all training examples don't have to fit in memory.\n",
    "\n",
    "[Good comparison of types of Gradient Descent and batch size](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)\n",
    "\n",
    "> Tip 1: A good default for batch size might be 32.  \n",
    "    - batch size is typically chosen between 1 and a few hundreds, e.g. batch size = 32 is a good default value, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the levers we can tweek are the optimizers which control how the weights and biases are updated.\n",
    "\n",
    "For stochastic gradient descent, the weights are updated with a **constant** learning rate (alpha) after every record.  If we specify a batch size, the constant learning rate is multiplied by the gradient across the batch. \n",
    "\n",
    "Other optimizers, such as **Adam** (not an acronym) update the weights in different ways. For Adam,\n",
    "> A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds. [source](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be clear, backpropogation calculates the gradient for each weight and bias, in each layer, including the input layer, for each **batch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to be clear:\n",
    "\n",
    "For mini-batch gradient descent, we:\n",
    "    - pass in a specified random sample of our training set\n",
    "    - the set propogates forward through our network\n",
    "    - each node sums the input, adds a bias, and applies an activation function to pass to the next layer.\n",
    "    - We make predictions on the output layer, then calculate the loss for back propogation.\n",
    "    - We calculate the derivative of the loss with regard to each weight and bias.\n",
    "    - We multiply that derivative by a learning rate determined by our optimizer.\n",
    "    - We update our parameters.\n",
    "    - We repeat for each batch until all examples have been used.\n",
    "    - We progress to the next epoch.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backprop](img/ff-bb.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphic above can be a bit frustrating since it moves fast, but follow the progress as so:\n",
    "\n",
    "Forward propogation with the **blue** tinted arrows computes the output of each layer: i.e. a summation and activation.\n",
    "\n",
    "Backprop calculates the partial derivative (**green** circles) for each weight (**brown** line) and bias.\n",
    "\n",
    "Then the optimizer multiplies a **learning rate** ($\\eta$) to each partial derivative to calculate a new weight which will be applied to the next batch that passes through."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
